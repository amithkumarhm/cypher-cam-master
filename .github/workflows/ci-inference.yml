# .github/workflows/ci-inference.yml
name: Inference Service CI/CD

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'services/inference/**'
      - '.github/workflows/ci-inference.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'services/inference/**'

env:
  PYTHON_VERSION: '3.11'

jobs:
  test:
    name: Test Inference Service
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        lfs: true

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
        cache: 'pip'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y libgl1-mesa-glx libglib2.0-0

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r services/inference/requirements.txt
        pip install pytest pytest-asyncio pytest-cov black flake8 mypy

    - name: Run type checking
      run: |
        cd services/inference
        mypy src/ --ignore-missing-imports

    - name: Run linting
      run: |
        cd services/inference
        black --check src/ tests/
        flake8 src/ tests/

    - name: Run unit tests
      run: |
        cd services/inference
        pytest tests/unit/ -v --cov=src --cov-report=xml

    - name: Run integration tests
      run: |
        cd services/inference
        pytest tests/integration/ -v --cov=src --cov-append

    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: services/inference/coverage.xml
        flags: inference
        name: inference-service

    - name: Test model loading
      run: |
        cd services/inference
        python -c "
        from src.computer_vision.models.model_manager import ModelManager
        from src.llm.models.model_loader import ModelLoader
        print('✓ Model imports successful')
        "

    - name: Build Docker image
      run: |
        cd services/inference
        docker build -t cypher-cam/inference:latest .

  model-test:
    name: Model Compatibility Test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        lfs: true

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
        pip install transformers accelerate
        pip install -r services/inference/requirements.txt

    - name: Test YOLO model loading
      run: |
        cd services/inference
        python -c "
        from ultralytics import YOLO
        try:
            model = YOLO('yolov8n.pt')
            print('✓ YOLO model loaded successfully')
        except Exception as e:
            print(f'✗ YOLO model loading failed: {e}')
        "

    - name: Test Transformers model loading
      run: |
        cd services/inference
        python -c "
        from transformers import AutoTokenizer, AutoModel
        try:
            tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
            model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
            print('✓ Transformers model loaded successfully')
        except Exception as e:
            print(f'✗ Transformers model loading failed: {e}')
        "

  training-test:
    name: Training Pipeline Test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install training dependencies
      run: |
        pip install peft bitsandbytes datasets
        pip install -r services/inference/requirements.txt

    - name: Test QLoRA imports
      run: |
        cd services/inference
        python -c "
        from src.llm_training.fine_tune_qlora import QLoRATrainer
        from src.training.llm_training.data_preparation import DataPreprocessor
        print('✓ Training imports successful')
        "

  docker-push:
    name: Push Docker Image
    runs-on: ubuntu-latest
    needs: [test, model-test, training-test]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Log in to Docker Hub
      uses: docker/login-action@v3
      with:
        username: ${{ secrets.DOCKERHUB_USERNAME }}
        password: ${{ secrets.DOCKERHUB_TOKEN }}

    - name: Build and push inference image
      uses: docker/build-push-action@v5
      with:
        context: services/inference
        push: true
        tags: |
          ${{ secrets.DOCKERHUB_USERNAME }}/cypher-cam-inference:latest
          ${{ secrets.DOCKERHUB_USERNAME }}/cypher-cam-inference:${{ github.sha }}
        cache-from: type=gha
        cache-to: type=gha,mode=max